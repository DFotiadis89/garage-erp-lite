ğŸš€ Project: garage-erp-lite
ğŸ“ˆ Monitoring Stack Time (Prometheus + Grafana + Loki)
Picked up right where we left off: â€œnext up: deploy the monitoring stackâ€.

Installed Helm on Windows (Winget worked fine after a restart).

Added repos:

prometheus-community

grafana

Ran helm repo update to make sure charts were fresh.

ğŸ›ï¸ Helm Install Adventures
Installed kube-prometheus-stack into a new monitoring namespace:

helm install kube-prom-stack prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace

Verified pods showed up fine in monitoring.

Grabbed Grafana's admin password with kubectl.

Port-forwarded Grafana to localhost:3000 and logged in.

Prometheus data source was already there.

âœ… Actually went smoother than expected.

ğŸªµ Loki (Logs) â€“ The Real Challenge
Added Grafana repo (just to be sure) and tried installing loki-stack without any extra config.

Immediate PVC Pending issue.

kubectl describe pvc revealed the default storageClass was set to "standard", which doesn't exist in k3d.

Solution: deleted the broken PVC and uninstalled the release to wipe everything clean.

âš¡ï¸ The Fix
Reinstalled with the proper storage class (local-path) and added volumeBindingMode=Immediate to avoid WaitForFirstConsumer problems:

helm install loki grafana/loki-stack
--namespace monitoring
--set loki.persistence.enabled=true
--set loki.persistence.size=5Gi
--set loki.persistence.storageClassName=local-path
--set loki.persistence.volumeBindingMode=Immediate
--set loki.config.schema_config.configs[0].store=boltdb-shipper
--set loki.config.schema_config.configs[0].object_store=filesystem
--set loki.config.schema_config.configs[0].schema=v11
--set loki.config.schema_config.configs[0].index.prefix=index_
--set loki.config.schema_config.configs[0].index.period=24h
--set loki.storage_config.boltdb_shipper.active_index_directory=/var/loki/index
--set loki.storage_config.boltdb_shipper.cache_location=/var/loki/boltdb-cache
--set loki.storage_config.filesystem.directory=/var/loki/chunks

Confirmed PVC bound with local-path.

Loki pod moved from Pending to Running.

Ran a wget test from inside the Grafana pod to confirm Loki was returning "ready".

ğŸ§ª Grafana Setup
Added Loki as a data source using the in-cluster service DNS:

http://loki.monitoring.svc.cluster.local:3100

"Save & Test" succeeded.

Ran basic queries in Explore and started seeing logs show up.

âš ï¸ The Classic Loki/Grafana Weirdness
Grafanaâ€™s "Logs Volume" graph kept failing with parse errors if we used {} or too-broad matchers.

Found out Loki was enforcing require_label_matchers_for=all in its config, rejecting empty selectors.

Realized: Grafanaâ€™s Explore Volume metric tries to run sum/count_over_time over too-broad queries Loki will refuse.

Resolved by using narrower queries like:

{node_name="k3d-garage-dev-server"}

âœ… Logs immediately appeared in Explore.

ğŸ’¡ Learnings & Fixes
PVCs are immutable in Kubernetesâ€”if you mess up storageClass you need to delete and recreate.

Helm install commands can get messy with too many --set flags, next step is to save them to values.yaml for Git.

Loki production Helm charts enforce label matchers to avoid cluster-killing queries.

Grafanaâ€™s Explore Volume graph fails with parse errors if your labels are too vague.

ğŸ•’ Session runtime: Who even knows at this point
â˜• Caffeine level: Prometheus tier
ğŸ’¡ Next up: Clean up Helm values into files for Git tracking