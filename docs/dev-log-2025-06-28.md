🚀 Project: garage-erp-lite
📈 Monitoring Stack Time (Prometheus + Grafana + Loki)
Picked up right where we left off: “next up: deploy the monitoring stack”.

Installed Helm on Windows (Winget worked fine after a restart).

Added repos:

prometheus-community

grafana

Ran helm repo update to make sure charts were fresh.

🎛️ Helm Install Adventures
Installed kube-prometheus-stack into a new monitoring namespace:

helm install kube-prom-stack prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace

Verified pods showed up fine in monitoring.

Grabbed Grafana's admin password with kubectl.

Port-forwarded Grafana to localhost:3000 and logged in.

Prometheus data source was already there.

✅ Actually went smoother than expected.

🪵 Loki (Logs) – The Real Challenge
Added Grafana repo (just to be sure) and tried installing loki-stack without any extra config.

Immediate PVC Pending issue.

kubectl describe pvc revealed the default storageClass was set to "standard", which doesn't exist in k3d.

Solution: deleted the broken PVC and uninstalled the release to wipe everything clean.

⚡️ The Fix
Reinstalled with the proper storage class (local-path) and added volumeBindingMode=Immediate to avoid WaitForFirstConsumer problems:

helm install loki grafana/loki-stack
--namespace monitoring
--set loki.persistence.enabled=true
--set loki.persistence.size=5Gi
--set loki.persistence.storageClassName=local-path
--set loki.persistence.volumeBindingMode=Immediate
--set loki.config.schema_config.configs[0].store=boltdb-shipper
--set loki.config.schema_config.configs[0].object_store=filesystem
--set loki.config.schema_config.configs[0].schema=v11
--set loki.config.schema_config.configs[0].index.prefix=index_
--set loki.config.schema_config.configs[0].index.period=24h
--set loki.storage_config.boltdb_shipper.active_index_directory=/var/loki/index
--set loki.storage_config.boltdb_shipper.cache_location=/var/loki/boltdb-cache
--set loki.storage_config.filesystem.directory=/var/loki/chunks

Confirmed PVC bound with local-path.

Loki pod moved from Pending to Running.

Ran a wget test from inside the Grafana pod to confirm Loki was returning "ready".

🧪 Grafana Setup
Added Loki as a data source using the in-cluster service DNS:

http://loki.monitoring.svc.cluster.local:3100

"Save & Test" succeeded.

Ran basic queries in Explore and started seeing logs show up.

⚠️ The Classic Loki/Grafana Weirdness
Grafana’s "Logs Volume" graph kept failing with parse errors if we used {} or too-broad matchers.

Found out Loki was enforcing require_label_matchers_for=all in its config, rejecting empty selectors.

Realized: Grafana’s Explore Volume metric tries to run sum/count_over_time over too-broad queries Loki will refuse.

Resolved by using narrower queries like:

{node_name="k3d-garage-dev-server"}

✅ Logs immediately appeared in Explore.

💡 Learnings & Fixes
PVCs are immutable in Kubernetes—if you mess up storageClass you need to delete and recreate.

Helm install commands can get messy with too many --set flags, next step is to save them to values.yaml for Git.

Loki production Helm charts enforce label matchers to avoid cluster-killing queries.

Grafana’s Explore Volume graph fails with parse errors if your labels are too vague.

🕒 Session runtime: Who even knows at this point
☕ Caffeine level: Prometheus tier
💡 Next up: Clean up Helm values into files for Git tracking